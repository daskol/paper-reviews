# Overcoming catastrophic forgetting in neural networks

***Kirkpatrick J. et al, 2017***

Пожалуй, главная ценность статьи заключается в том, что авторы проводят параллели между глубинным обучением и нейробиологией в проблеме катастрофического забывания(catastrofic forgetting problem).
Авторы замечают, что согласно некоторым нейробиологическим представлениям каждый синапс в мозге млекопитающего несёт не только силу связи, но и кодирует её вариацию в амплитуде постсинаптического возбужедения.

Такая мотивация позволила предложить адаптивный регуляризатор, который управлял бы скоростью обучения(learning rate) каждой скрытой переменной.
Адаптивность заключается же в том, что скорость обучения обратно пропорциональна вариации весов.

Предложенный Elastic Weight Consolidation(EWC) регулярзитор может быть обоснован байесовсем рассуждением.
Пусть есть последовательность различных задач.
Тогда MAP-оценка параметров на следующей задаче модифицируется таким образом, что априорные вероятности весов заменяются оценкой, полученной на предыдущей задаче.
Предполагая i.i.d. для весов, получаем квадратичный по весам регуляризатор EWC, пропорциональный диагональной матрицы Фишера.

По существу регуляризатор пинит веса в некоторых точках ограничивает градиенты весов.
Аналитическая форма функции потерь напоминает лагранжиан в модели Гинзбурга-Ландау.
Учитывая комментарий, о том, как работают на их задача L2 регуляризатор можно сделать вывод о том, что создаются "долины", так что часть модели оказывается в равновесии.
Кажется, можно проверить этот факт, разбив L2 регуляризатор на два с различными весами.
Вообще говоря, такой подход напоминает построение минимальной модели нейросети, способной классифицировать MNIST, а EWC регуляризатор способ покомпактнее упаковать несколько моделей вместе.

В работе описано два эксперимента: обычение с учителем на MNIST, где после каждой задачи все пиксели изображения случайно переставлялись; и агент для игр Atari.
Первый эксперимент показал, что EWC заметно выигрывает, и что похожесть информационность матриц сравнивается при углублении сети.
Во втором экспермиенте DQN-агенту случайно подавали десять различных игр с повторением, на которых агент показывает неплохие результаты.
